<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Colin Fraser">
<meta name="dcterms.date" content="2024-11-14">

<title>Estimating how many there are of something when you can’t see them all perfectly – Colin Fraser</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-6QW9PMVM0W"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-6QW9PMVM0W', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Colin Fraser</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="mailto://colin.r.fraser@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/colin_fraser"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/colin-fraser/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/@colin.fraser"> <i class="bi bi-medium" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/colin-fraser"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Estimating how many there are of something when you can’t see them all perfectly</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Colin Fraser </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 14, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>I wrote a tweet recently complaining about how it’s hard to estimate how many of what kind of posts there are.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
You guys really have no idea how hard it is to estimate how many of what kind of posts there are
</p>
— Colin Fraser (<span class="citation" data-cites="colin_fraser">@colin_fraser</span>) <a href="https://twitter.com/colin_fraser/status/1846684000808800763?ref_src=twsrc%5Etfw">October 16, 2024</a>
</blockquote>
<script async="" src="https://platform.x.com/widgets.js" charset="utf-8"></script>
<p>This was a rare window on X into my professional life, which involves estimating how many posts are against the rules on the social media app that I work for. For many reasons, this turns out to be significantly harder than you might initially expect. This blog post is about just one of those reasons, a particular statistical quirk that arises in estimating a prevalence under measurement error. The tl;dr is that if there is any measurement error whatsoever, then a naive estimation procedure is almost guaranteed to produce a biased estimate of prevalence. This might be a bit surprising, because that’s emphatically not the case most of the time: usually, when you have measurements which may contain errors, you can expect that the positive errors and the negative errors will cancel out, leading to unbiased estimation. The result of measurement error is noise, but not <em>bias</em>. This fact is a bedrock of most of applied statistics. But in the case where observations are binary—posts are either against the rules or they’re not—the situation is different, and it’s sadly the case that any measurement error at all introduces a fairly complicated form of bias.</p>
<section id="the-basic-problem-of-estimating-prevalence" class="level3">
<h3 class="anchored" data-anchor-id="the-basic-problem-of-estimating-prevalence">The basic problem of estimating prevalence</h3>
<p>Say you want to estimate how much there is of some kind of thing. There’s a big set of things, some of which satisfy some property, and others of which don’t, and you want to know what fraction of them have the property. This fraction is the <em>prevalence</em> of the property, which I’ll denote as <span class="math inline">\(\mu\)</span>.</p>
<p>The most straightforward way to do this in theory would be to look at each thing and tally up how many of them have the property, but this is often infeasible. I can’t personally inspect every post and decide whether it violates the rules. Instead, you have some process that assigns a <em>label</em> to each thing. The label is meant to indicate whether the thing has the property, but sometimes it’s wrong.</p>
<p>I’m going to introduce some simple notation. Let <span class="math inline">\(Y\)</span> be the true value of a randomly selected object, with <span class="math inline">\(Y=1\)</span> indicating that the object has the property and <span class="math inline">\(Y=0\)</span> otherwise, and let <span class="math inline">\(L\)</span> be its label. If <span class="math inline">\(Y=1\)</span>, I’ll call the object an “actual positive”, and if <span class="math inline">\(L = 1\)</span>, I’ll call it an “apparent positive”. The goal is to estimate the <em>true prevalence</em> <span class="math inline">\(\mu = P(Y=1)\)</span>. By the way, a nice thing about binary variables like this is that we can also write <span class="math inline">\(P(Y=1)=E[Y]\)</span> which is how I will primarily describe <span class="math inline">\(\mu\)</span> here, but it’s useful to remember that these are the same.</p>
<p>A natural inclination is to just treat <span class="math inline">\(L\)</span> as a proxy for <span class="math inline">\(Y\)</span> and estimate the <em>apparent prevalence</em> <span class="math inline">\(E[L]=P(L=1)\)</span>, which I’ll denote in this post as <span class="math inline">\(\ell\)</span> (for <strong>l</strong>abel).</p>
<p>This post is about why that doesn’t work.</p>
<section id="a-few-examples-of-this-problem-in-practice" class="level4">
<h4 class="anchored" data-anchor-id="a-few-examples-of-this-problem-in-practice">A few examples of this problem in practice</h4>
<p>I’m describing this all very abstractly, and the reason is that this situation actually comes up all the time, in all kinds of different ways. Here are a few examples.</p>
<p>You could be trying to estimate the fraction of people in a population who carry some disease. You can’t observe every person in the population, and even if you could, you can’t know for sure whether any person actually has the disease. All you can know is the outcome of a test which is administered to them. The test is imperfect, and occasionally produces false positives and false negatives. In this case, <span class="math inline">\(Y\)</span> describes whether a randomly selected person actually has the disease, and <span class="math inline">\(L\)</span> indicates whether they test positive.</p>
<p>Or maybe <span class="math inline">\(\mu\)</span> is the fraction of examples from some <a href="https://github.com/vectara/hallucination-leaderboard">LLM benchmark</a> which contain a hallucination. Such a benchmark might have thousands of prompts, and so it might be infeasible to manually review them all and assess whether they contain hallucinations. To deal with this, many benchmarks of this type use another LLM—ideally one that is more powerful in some sense—to evaluate whether responses contain hallucinations. But this evaluator LLM itself can be error-prone: it can falsely indicate that hallucination-free text contains hallucinations (a false positive), and vice versa (a false negative).</p>
<p>Perhaps instead, <span class="math inline">\(\mu\)</span> is <a href="https://twitter.com/XData/status/1750280284635824485">the amount of discussion on a microblogging app about The Academy Awards</a>. You can’t look at every single post, but you can for example count up how many posts contain the string “Oscar”. Of course, this will falsely count posts discussing Oscar The Grouch (false positives), and it will falsely miss posts which don’t refer to the awards by name at all.</p>
<p>In my specific case, <span class="math inline">\(\mu\)</span> is the fraction of all posts on a particular social media app which violate the rules. To determine this, we show a sample of posts to people for review, who label each as either violating or not. But sometimes the reviewers make a mistake.</p>
<p>All of these situations are abstractly the same. They all involve trying to estimate some true prevalence <span class="math inline">\(\mu\)</span> by observing examples of possibly imperfect labels <span class="math inline">\(L\)</span>. Again, the natural inclination is to just use an imperfect label <span class="math inline">\(L\)</span> like it’s a true value <span class="math inline">\(Y\)</span>. Even if you’re aware that your labels are imperfect, maybe somehow or another the true positives and false positives will cancel out in the end, leading to something which might be noisy but is at least right on average. This does happen with other forms of measurement error. Unfortunately, it doesn’t happen here.</p>
</section>
</section>
<section id="quantifying-label-quality" class="level3">
<h3 class="anchored" data-anchor-id="quantifying-label-quality">Quantifying Label Quality</h3>
<p>To better understand what does happen, we need two important measures of label quality: the true positive rate (TPR) and the false positive rate (FPR). The TPR, which I’ll also denote by <span class="math inline">\(\alpha\)</span>, is the probability that an actual positive is correctly labeled. Using the notation from above, it can be written as <span class="math inline">\(\alpha = P(L=1|Y=1)\)</span>. This quantity goes by many names: when the labels come from a machine learning model, it’s often called the <em>recall</em>, and when they come from a medical test, it’s called the <em>sensitivity</em>. The FPR, denoted by <span class="math inline">\(\beta\)</span>, is the probability that an actual negative is falsely identified as an apparent positive: <span class="math inline">\(\beta = P(L=1|Y=0)\)</span>.</p>
<p>These two measures characterize imperfectness of the labeling process. A perfect labeler would have <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\beta = 0\)</span>. An imperfect labeler will have <span class="math inline">\(\alpha &lt; 1\)</span> and/or <span class="math inline">\(\beta &gt; 0\)</span>. It’s a fact that if the labels are better than coin flips, we must have <span class="math inline">\(\alpha &gt; \beta\)</span>. For the most part I’ll assume that this holds, but it will be interesting to think through what happens if it doesn’t.</p>
</section>
<section id="quantifying-the-bias" class="level3">
<h3 class="anchored" data-anchor-id="quantifying-the-bias">Quantifying the bias</h3>
<p>With these defined, it’s pretty straightforward to see that the apparent prevalence <span class="math inline">\(\ell\)</span> can be written as follows.</p>
<p><span class="math display">\[\begin{align*}
\ell &amp;= E[L] \\ &amp;= E[L | Y = 1] P (Y = 1) + E[L|Y = 0] P(Y=0) \\
&amp;= \alpha \mu + \beta (1-\mu) \\ &amp; = \beta + (\alpha - \beta) \mu
\end{align*}\]</span></p>
<p>This simple equation holds many important truths. Naturally, if we have a perfect labeler with <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\beta = 0\)</span>, it says that the apparent prevalence <span class="math inline">\(\ell\)</span> equals the true prevalence <span class="math inline">\(\mu\)</span>. But otherwise, it says that these can’t be equal in general. The apparent prevalence ends up differing from the actual prevalence, and the amount by which it differs depends on all of <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\alpha\)</span>, and <span class="math inline">\(\beta\)</span>. As a function of <span class="math inline">\(\mu\)</span>, we have a straight line with intercept <span class="math inline">\(\beta\)</span>, and slope <span class="math inline">\(\alpha - \beta\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="index_files/figure-html/fig1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>There is a single point, which I’ve labeled <span class="math inline">\(\mu'\)</span>, at which the actual prevalence is equal to the apparent prevalence, but for every other possible value of prevalence, the apparent prevalence differs. The magnitude and even the direction of this bias can take on a range of different values depending on the true prevalence. In a way, this is pretty disappointing news. It means that unless you have perfect labels, you’re virtually guaranteed to estimate prevalence incorrectly, and you can’t even say for sure in general how big the inaccuracy is.</p>
<p>There are a few other bits of insight that we can obtain by studying this graph. For one thing, the relationship between the true and apparent prevalence is always flatter than the 45 degree line. This means that for small values of the actual prevalence (anywhere to the left of <span class="math inline">\(\mu'\)</span>), we will tend to overestimate, and vice versa. If you have some sense of the approximate magnitude of the true prevalence, you can use this as a kind of rule of thumb to guess the direction of the bias, even if you don’t know the true and false positive rates for sure. If you’re trying to measure a very small prevalence with an imperfect test, you’re probably overestimating, and vice versa.</p>
<p>It also means that this estimation procedure will tend to understate the magnitude of <em>changes</em> in prevalence: when prevalence changes from <span class="math inline">\(\mu_0\)</span> to <span class="math inline">\(\mu_0 + \Delta\)</span>, the estimate will change by <span class="math inline">\((\alpha - \beta)\Delta\)</span>, which is strictly less (in absolute value) than <span class="math inline">\(\Delta\)</span>. The apparent prevalence is squished in between <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. In the extreme case where <span class="math inline">\(\alpha = \beta\)</span>, the line becomes flat, and you’ll end up estimating that prevalence is equal to <span class="math inline">\(\beta\)</span> on average no matter its true value. Given that a labeler with <span class="math inline">\(\alpha=\beta\)</span> is not better than a coin flip, it shouldn’t be surprising that labels generated in this way give no information about the true prevalence. Nonetheless, I’ve noticed in the real world that this tends to be a bit unintuitive. It’s tempting to expect that any labeling process will lead to estimates which are “directionally correct” even in the face of measurement error, but this shows that that’s not necessarily true! If the false positive rate is close enough to the true positive rate, the estimate becomes pure noise. In the perverse scenario where the FPR is <em>higher</em> than the TPR, the slope <span class="math inline">\(\alpha - \beta\)</span> is negative, and increases in the true prevalence lead to <em>decreases</em> in the apparent prevalence, and vice versa. You would hope never to find yourself in this scenario, but it’s not impossible, and it’s good to be aware of this possibility.</p>
<p>All of this is particularly problematic if your project is to track the prevalence of something over time. If you’re tracking the progress of some disease which currently has a small prevalence, for example, it means that small upticks in the estimated prevalence probably indicate larger increases in the underlying true prevalence. But the more the disease spreads, the smaller that bias gets, until eventually it may become negative. This is very annoying.</p>
<p>It also has implications for experimentation. Suppose you are testing some intervention which is intended to change the prevalence. You’ll do this by comparing the prevalence in a test group to the prevalence in a control group. But with imperfect labels, you will underestimate the magnitude of the difference between the two groups.</p>
<section id="formulation-in-terms-of-precision-and-recall" class="level4">
<h4 class="anchored" data-anchor-id="formulation-in-terms-of-precision-and-recall">Formulation in terms of precision and recall</h4>
<p>(This section is slightly wonky and you can safely skip over it.)</p>
<p>If the labels come from a machine learning model, it’s common to talk about the <em>precision</em> rather than the FPR. I think it’s better to use the FPR for reasons I’ll talk about momentarily, but there is a nice neat formula relating the precision and recall to prevalence, so I may as well include it. In terms of our notation, the precision, which I’ll denote by <span class="math inline">\(\pi\)</span>, can be written as <span class="math inline">\(\pi=P(Y=1|L=1)\)</span>, the probability that an apparent positive is actually positive. Applying Bayes’ theorem, we can write the following.</p>
<p><span class="math display">\[\begin{align*}
\pi &amp;= \frac{P(L=1|Y=1)P(Y=1)}{P(L=1)} \\
&amp;= \frac{\alpha \mu} {\ell}
\end{align*}\]</span></p>
<p>Rearranging gives an expression for <span class="math inline">\(\ell\)</span> in terms of the precision and recall.</p>
<p><span class="math display">\[\ell = \frac{\alpha \mu}{\pi}\]</span></p>
<p>The reason I don’t like this framing is that the precision itself is sneakily a function of the prevalence, whereas one can somewhat reasonably assume that the TPR and FPR are not (though this is also an assumption). This is clear from the previous derivation: <span class="math inline">\(\pi = \frac{\alpha \mu} {\ell}\)</span>. It’s also easy to see if you think about the extreme cases: if the prevalence is <span class="math inline">\(0\)</span> then the precision must be <span class="math inline">\(0\)</span>, since any apparent positive will not be an actual positive. Similarly, if the prevalence is <span class="math inline">\(1\)</span> then the precision must be <span class="math inline">\(1\)</span>. So the previous equation gives a misleadingly simplistic perspective on <span class="math inline">\(\ell\)</span> as a function of prevalence.</p>
<p>Nonetheless it does provide some interesting alternative ways of looking at this situation. If <span class="math inline">\(\ell = \frac{\alpha \mu}{\pi}\)</span>, it follows that <span class="math inline">\(\ell = \mu\)</span> if and only if <span class="math inline">\(\alpha = \pi\)</span>, if the precision is equal to the recall. If labels are perfect then this will be true: precision and recall will both be <span class="math inline">\(1\)</span>. Otherwise, for a given recall (and assuming a fixed FPR), this will only be true at some specific value of prevalence—in particular, the value <span class="math inline">\(\mu '\)</span> from before.</p>
<p>This also provides a way of thinking about how the quantifier bias relates to the precision-recall trade-off. For a high precision labeler, the ratio <span class="math inline">\(\alpha/\pi\)</span> is small, and so the apparent prevalence will understate the true prevalence. It makes sense. A high precision classifier only produces apparent positives when it’s very sure, overlooking less certain cases, leading to an underestimation of the true prevalence. The opposite is true for a high recall labeler: <span class="math inline">\(\alpha/\pi\)</span> is large, and so <span class="math inline">\(\ell &gt; \mu\)</span>. With a high recall labeler we are casting a wide net, including many actual negatives in our estimation of prevalence.</p>
</section>
</section>
<section id="some-discussion-and-proposed-solutions-from-the-literature" class="level3">
<h3 class="anchored" data-anchor-id="some-discussion-and-proposed-solutions-from-the-literature">Some discussion and proposed solutions from the literature</h3>
<p>When the source of the imperfect labels is a machine learning model, the problem of estimating the true prevalence has sometimes been called <a href="https://en.wikipedia.org/wiki/Quantification_(machine_learning)">Quantification</a>—a secret third brother of the classical supervised tasks of Regression and Classification. In this context, the naive approach using <span class="math inline">\(\ell\)</span> as a proxy for the true prevalence has been called the “Classify And Count” method by Forman (2008), who proposes an alternative method which corrects the bias, appropriately called the “Adjusted Classify-And-Count” (ACC) method. Given <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, the ACC estimator is obtained by simply solving the equation <span class="math inline">\(\ell = \beta + (\alpha - \beta) \mu\)</span> for <span class="math inline">\(\mu\)</span>, leading to:</p>
<p><span class="math display">\[\hat \mu_{ACC}=\frac{\ell - \beta}{\alpha - \beta}\]</span></p>
<p>This is not the first time this problem has been noticed. Rogan and Gladen (1978) discuss how the prevalence of diseases can be incorrectly estimated when we rely on imperfect tests, and propose the following correction, thereafter known as the Rogan-Gladen estimator.</p>
<p><span class="math display">\[\hat \mu _{RG} = \frac{\ell + \text{Specificity} - 1}{\text{Sensitivity} + \text{Specificity} - 1}\]</span></p>
<p>“Specificity” and “sensitivity” are more frequently used to describe medical tests than true and false positive rates, but it turns out this is actually exactly equivalent to the ACC correction. Sensitivity is just a synonym for recall, and the definition of specificity is <span class="math inline">\(P(L=0 | Y = 0)\)</span>, which happens to be equal to <span class="math inline">\(1 - \beta\)</span>. Substituting these in to <span class="math inline">\(\hat \mu_{RG}\)</span>, you get the exact formula for <span class="math inline">\(\hat \mu _{ACC}\)</span>. Funny how things are discovered and rediscovered. I’m sure someone must have noticed this connection before, but I haven’t actually seen it written anywhere.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>There’s a lot more to talk about here. There are many other methods which are more sophisticated than ACC/Rogan-Gladen, and we haven’t even touched on the notion of a confidence interval here. And the solution above really does just kick the can down the road: it’s no easier in general to obtain reliable estimates of the TPR and FPR than it is to estimate prevalence directly. There’s a tricky resource optimization problem lurking here: is it cheaper to get high quality TPR and FPR estimates in order to build a Quantifier, or would you be better off just estimating prevalence directly by more traditional means? This is also not the only thing that makes it hard to estimate how many of what kind of posts there are. All kinds of other issues like severe class imbalance, non-response bias, and others all arise and compete and interact with each other to make it a very complicated task.</p>
<p>These are all perhaps topics for a future post, but the goal of this post has just been to raise awareness of this one tricky issue that I’ve rarely seen discussed, especially given its apparent ubiquity across different fields, and its annoyingness in my own day-to-day life.</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ul>
<li><p>Forman G. Quantifying counts and costs via classification. Data Mining and Knowledge Discovery. 2008 Oct;17:164-206.</p></li>
<li><p>Rogan WJ, Gladen B. Estimating prevalence from the results of a screening test. American journal of epidemiology. 1978 Jan 1;107(1):71-6.</p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/colin-fraser\.net");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>