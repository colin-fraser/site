<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Colin Fraser</title>
<link>https://colin-fraser.net/</link>
<atom:link href="https://colin-fraser.net/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Thu, 14 Nov 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Estimating how many there are of something when you can’t see them all perfectly</title>
  <dc:creator>Colin Fraser</dc:creator>
  <link>https://colin-fraser.net/posts/2024-11-12-estimating-how-many-there-are-of-something-using-a-classifier/</link>
  <description><![CDATA[ 




<p>I wrote a tweet recently complaining about how it’s hard to estimate how many of what kind of posts there are.</p>
<blockquote class="twitter-tweet blockquote">
<p lang="en" dir="ltr">
You guys really have no idea how hard it is to estimate how many of what kind of posts there are
</p>
— Colin Fraser (<span class="citation" data-cites="colin_fraser">@colin_fraser</span>) <a href="https://twitter.com/colin_fraser/status/1846684000808800763?ref_src=twsrc%5Etfw">October 16, 2024</a>
</blockquote>
<script async="" src="https://platform.x.com/widgets.js" charset="utf-8"></script>
<p>This was a rare window on X into my professional life, which involves estimating how many posts are against the rules on the social media app that I work for. For many reasons, this turns out to be significantly harder than you might initially expect. This blog post is about just one of those reasons, a particular statistical quirk that arises in estimating a prevalence under measurement error. The tl;dr is that if there is any measurement error whatsoever, then a naive estimation procedure is almost guaranteed to produce a biased estimate of prevalence. This might be a bit surprising, because that’s emphatically not the case most of the time: usually, when you have measurements which may contain errors, you can expect that the positive errors and the negative errors will cancel out, leading to unbiased estimation. The result of measurement error is noise, but not <em>bias</em>. This fact is a bedrock of most of applied statistics. But in the case where observations are binary—posts are either against the rules or they’re not—the situation is different, and it’s sadly the case that any measurement error at all introduces a fairly complicated form of bias.</p>
<section id="the-basic-problem-of-estimating-prevalence" class="level3">
<h3 class="anchored" data-anchor-id="the-basic-problem-of-estimating-prevalence">The basic problem of estimating prevalence</h3>
<p>Say you want to estimate how much there is of some kind of thing. There’s a big set of things, some of which satisfy some property, and others of which don’t, and you want to know what fraction of them have the property. This fraction is the <em>prevalence</em> of the property, which I’ll denote as <img src="https://latex.codecogs.com/png.latex?%5Cmu">.</p>
<p>The most straightforward way to do this in theory would be to look at each thing and tally up how many of them have the property, but this is often infeasible. I can’t personally inspect every post and decide whether it violates the rules. Instead, you have some process that assigns a <em>label</em> to each thing. The label is meant to indicate whether the thing has the property, but sometimes it’s wrong.</p>
<p>I’m going to introduce some simple notation. Let <img src="https://latex.codecogs.com/png.latex?Y"> be the true value of a randomly selected object, with <img src="https://latex.codecogs.com/png.latex?Y=1"> indicating that the object has the property and <img src="https://latex.codecogs.com/png.latex?Y=0"> otherwise, and let <img src="https://latex.codecogs.com/png.latex?L"> be its label. If <img src="https://latex.codecogs.com/png.latex?Y=1">, I’ll call the object an “actual positive”, and if <img src="https://latex.codecogs.com/png.latex?L%20=%201">, I’ll call it an “apparent positive”. The goal is to estimate the <em>true prevalence</em> <img src="https://latex.codecogs.com/png.latex?%5Cmu%20=%20P(Y=1)">. By the way, a nice thing about binary variables like this is that we can also write <img src="https://latex.codecogs.com/png.latex?P(Y=1)=E%5BY%5D"> which is how I will primarily describe <img src="https://latex.codecogs.com/png.latex?%5Cmu"> here, but it’s useful to remember that these are the same.</p>
<p>A natural inclination is to just treat <img src="https://latex.codecogs.com/png.latex?L"> as a proxy for <img src="https://latex.codecogs.com/png.latex?Y"> and estimate the <em>apparent prevalence</em> <img src="https://latex.codecogs.com/png.latex?E%5BL%5D=P(L=1)">, which I’ll denote in this post as <img src="https://latex.codecogs.com/png.latex?%5Cell"> (for <strong>l</strong>abel).</p>
<p>This post is about why that doesn’t work.</p>
<section id="a-few-examples-of-this-problem-in-practice" class="level4">
<h4 class="anchored" data-anchor-id="a-few-examples-of-this-problem-in-practice">A few examples of this problem in practice</h4>
<p>I’m describing this all very abstractly, and the reason is that this situation actually comes up all the time, in all kinds of different ways. Here are a few examples.</p>
<p>You could be trying to estimate the fraction of people in a population who carry some disease. You can’t observe every person in the population, and even if you could, you can’t know for sure whether any person actually has the disease. All you can know is the outcome of a test which is administered to them. The test is imperfect, and occasionally produces false positives and false negatives. In this case, <img src="https://latex.codecogs.com/png.latex?Y"> describes whether a randomly selected person actually has the disease, and <img src="https://latex.codecogs.com/png.latex?L"> indicates whether they test positive.</p>
<p>Or maybe <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is the fraction of examples from some <a href="https://github.com/vectara/hallucination-leaderboard">LLM benchmark</a> which contain a hallucination. Such a benchmark might have thousands of prompts, and so it might be infeasible to manually review them all and assess whether they contain hallucinations. To deal with this, many benchmarks of this type use another LLM—ideally one that is more powerful in some sense—to evaluate whether responses contain hallucinations. But this evaluator LLM itself can be error-prone: it can falsely indicate that hallucination-free text contains hallucinations (a false positive), and vice versa (a false negative).</p>
<p>Perhaps instead, <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is <a href="https://twitter.com/XData/status/1750280284635824485">the amount of discussion on a microblogging app about The Academy Awards</a>. You can’t look at every single post, but you can for example count up how many posts contain the string “Oscar”. Of course, this will falsely count posts discussing Oscar The Grouch (false positives), and it will falsely miss posts which don’t refer to the awards by name at all.</p>
<p>In my specific case, <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is the fraction of all posts on a particular social media app which violate the rules. To determine this, we show a sample of posts to people for review, who label each as either violating or not. But sometimes the reviewers make a mistake.</p>
<p>All of these situations are abstractly the same. They all involve trying to estimate some true prevalence <img src="https://latex.codecogs.com/png.latex?%5Cmu"> by observing examples of possibly imperfect labels <img src="https://latex.codecogs.com/png.latex?L">. Again, the natural inclination is to just use an imperfect label <img src="https://latex.codecogs.com/png.latex?L"> like it’s a true value <img src="https://latex.codecogs.com/png.latex?Y">. Even if you’re aware that your labels are imperfect, maybe somehow or another the true positives and false positives will cancel out in the end, leading to something which might be noisy but is at least right on average. This does happen with other forms of measurement error. Unfortunately, it doesn’t happen here.</p>
</section>
</section>
<section id="quantifying-label-quality" class="level3">
<h3 class="anchored" data-anchor-id="quantifying-label-quality">Quantifying Label Quality</h3>
<p>To better understand what does happen, we need two important measures of label quality: the true positive rate (TPR) and the false positive rate (FPR). The TPR, which I’ll also denote by <img src="https://latex.codecogs.com/png.latex?%5Calpha">, is the probability that an actual positive is correctly labeled. Using the notation from above, it can be written as <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20P(L=1%7CY=1)">. This quantity goes by many names: when the labels come from a machine learning model, it’s often called the <em>recall</em>, and when they come from a medical test, it’s called the <em>sensitivity</em>. The FPR, denoted by <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, is the probability that an actual negative is falsely identified as an apparent positive: <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%20P(L=1%7CY=0)">.</p>
<p>These two measures characterize imperfectness of the labeling process. A perfect labeler would have <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%201"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%200">. An imperfect labeler will have <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%3C%201"> and/or <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20%3E%200">. It’s a fact that if the labels are better than coin flips, we must have <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%3E%20%5Cbeta">. For the most part I’ll assume that this holds, but it will be interesting to think through what happens if it doesn’t.</p>
</section>
<section id="quantifying-the-bias" class="level3">
<h3 class="anchored" data-anchor-id="quantifying-the-bias">Quantifying the bias</h3>
<p>With these defined, it’s pretty straightforward to see that the apparent prevalence <img src="https://latex.codecogs.com/png.latex?%5Cell"> can be written as follows.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cell%20&amp;=%20E%5BL%5D%20%5C%5C%20&amp;=%20E%5BL%20%7C%20Y%20=%201%5D%20P%20(Y%20=%201)%20+%20E%5BL%7CY%20=%200%5D%20P(Y=0)%20%5C%5C%0A&amp;=%20%5Calpha%20%5Cmu%20+%20%5Cbeta%20(1-%5Cmu)%20%5C%5C%20&amp;%20=%20%5Cbeta%20+%20(%5Calpha%20-%20%5Cbeta)%20%5Cmu%0A%5Cend%7Balign*%7D"></p>
<p>This simple equation holds many important truths. Naturally, if we have a perfect labeler with <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%201"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta%20=%200">, it says that the apparent prevalence <img src="https://latex.codecogs.com/png.latex?%5Cell"> equals the true prevalence <img src="https://latex.codecogs.com/png.latex?%5Cmu">. But otherwise, it says that these can’t be equal in general. The apparent prevalence ends up differing from the actual prevalence, and the amount by which it differs depends on all of <img src="https://latex.codecogs.com/png.latex?%5Cmu">, <img src="https://latex.codecogs.com/png.latex?%5Calpha">, and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. As a function of <img src="https://latex.codecogs.com/png.latex?%5Cmu">, we have a straight line with intercept <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, and slope <img src="https://latex.codecogs.com/png.latex?%5Calpha%20-%20%5Cbeta">.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://colin-fraser.net/posts/2024-11-12-estimating-how-many-there-are-of-something-using-a-classifier/index_files/figure-html/fig1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="768"></p>
</figure>
</div>
</div>
</div>
<p>There is a single point, which I’ve labeled <img src="https://latex.codecogs.com/png.latex?%5Cmu'">, at which the actual prevalence is equal to the apparent prevalence, but for every other possible value of prevalence, the apparent prevalence differs. The magnitude and even the direction of this bias can take on a range of different values depending on the true prevalence. In a way, this is pretty disappointing news. It means that unless you have perfect labels, you’re virtually guaranteed to estimate prevalence incorrectly, and you can’t even say for sure in general how big the inaccuracy is.</p>
<p>There are a few other bits of insight that we can obtain by studying this graph. For one thing, the relationship between the true and apparent prevalence is always flatter than the 45 degree line. This means that for small values of the actual prevalence (anywhere to the left of <img src="https://latex.codecogs.com/png.latex?%5Cmu'">), we will tend to overestimate, and vice versa. If you have some sense of the approximate magnitude of the true prevalence, you can use this as a kind of rule of thumb to guess the direction of the bias, even if you don’t know the true and false positive rates for sure. If you’re trying to measure a very small prevalence with an imperfect test, you’re probably overestimating, and vice versa.</p>
<p>It also means that this estimation procedure will tend to understate the magnitude of <em>changes</em> in prevalence: when prevalence changes from <img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> to <img src="https://latex.codecogs.com/png.latex?%5Cmu_0%20+%20%5CDelta">, the estimate will change by <img src="https://latex.codecogs.com/png.latex?(%5Calpha%20-%20%5Cbeta)%5CDelta">, which is strictly less (in absolute value) than <img src="https://latex.codecogs.com/png.latex?%5CDelta">. The apparent prevalence is squished in between <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. In the extreme case where <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20%5Cbeta">, the line becomes flat, and you’ll end up estimating that prevalence is equal to <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> on average no matter its true value. Given that a labeler with <img src="https://latex.codecogs.com/png.latex?%5Calpha=%5Cbeta"> is not better than a coin flip, it shouldn’t be surprising that labels generated in this way give no information about the true prevalence. Nonetheless, I’ve noticed in the real world that this tends to be a bit unintuitive. It’s tempting to expect that any labeling process will lead to estimates which are “directionally correct” even in the face of measurement error, but this shows that that’s not necessarily true! If the false positive rate is close enough to the true positive rate, the estimate becomes pure noise. In the perverse scenario where the FPR is <em>higher</em> than the TPR, the slope <img src="https://latex.codecogs.com/png.latex?%5Calpha%20-%20%5Cbeta"> is negative, and increases in the true prevalence lead to <em>decreases</em> in the apparent prevalence, and vice versa. You would hope never to find yourself in this scenario, but it’s not impossible, and it’s good to be aware of this possibility.</p>
<p>All of this is particularly problematic if your project is to track the prevalence of something over time. If you’re tracking the progress of some disease which currently has a small prevalence, for example, it means that small upticks in the estimated prevalence probably indicate larger increases in the underlying true prevalence. But the more the disease spreads, the smaller that bias gets, until eventually it may become negative. This is very annoying.</p>
<p>It also has implications for experimentation. Suppose you are testing some intervention which is intended to change the prevalence. You’ll do this by comparing the prevalence in a test group to the prevalence in a control group. But with imperfect labels, you will underestimate the magnitude of the difference between the two groups.</p>
<section id="formulation-in-terms-of-precision-and-recall" class="level4">
<h4 class="anchored" data-anchor-id="formulation-in-terms-of-precision-and-recall">Formulation in terms of precision and recall</h4>
<p>(This section is slightly wonky and you can safely skip over it.)</p>
<p>If the labels come from a machine learning model, it’s common to talk about the <em>precision</em> rather than the FPR. I think it’s better to use the FPR for reasons I’ll talk about momentarily, but there is a nice neat formula relating the precision and recall to prevalence, so I may as well include it. In terms of our notation, the precision, which I’ll denote by <img src="https://latex.codecogs.com/png.latex?%5Cpi">, can be written as <img src="https://latex.codecogs.com/png.latex?%5Cpi=P(Y=1%7CL=1)">, the probability that an apparent positive is actually positive. Applying Bayes’ theorem, we can write the following.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Cpi%20&amp;=%20%5Cfrac%7BP(L=1%7CY=1)P(Y=1)%7D%7BP(L=1)%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B%5Calpha%20%5Cmu%7D%20%7B%5Cell%7D%0A%5Cend%7Balign*%7D"></p>
<p>Rearranging gives an expression for <img src="https://latex.codecogs.com/png.latex?%5Cell"> in terms of the precision and recall.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cell%20=%20%5Cfrac%7B%5Calpha%20%5Cmu%7D%7B%5Cpi%7D"></p>
<p>The reason I don’t like this framing is that the precision itself is sneakily a function of the prevalence, whereas one can somewhat reasonably assume that the TPR and FPR are not (though this is also an assumption). This is clear from the previous derivation: <img src="https://latex.codecogs.com/png.latex?%5Cpi%20=%20%5Cfrac%7B%5Calpha%20%5Cmu%7D%20%7B%5Cell%7D">. It’s also easy to see if you think about the extreme cases: if the prevalence is <img src="https://latex.codecogs.com/png.latex?0"> then the precision must be <img src="https://latex.codecogs.com/png.latex?0">, since any apparent positive will not be an actual positive. Similarly, if the prevalence is <img src="https://latex.codecogs.com/png.latex?1"> then the precision must be <img src="https://latex.codecogs.com/png.latex?1">. So the previous equation gives a misleadingly simplistic perspective on <img src="https://latex.codecogs.com/png.latex?%5Cell"> as a function of prevalence.</p>
<p>Nonetheless it does provide some interesting alternative ways of looking at this situation. If <img src="https://latex.codecogs.com/png.latex?%5Cell%20=%20%5Cfrac%7B%5Calpha%20%5Cmu%7D%7B%5Cpi%7D">, it follows that <img src="https://latex.codecogs.com/png.latex?%5Cell%20=%20%5Cmu"> if and only if <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20%5Cpi">, if the precision is equal to the recall. If labels are perfect then this will be true: precision and recall will both be <img src="https://latex.codecogs.com/png.latex?1">. Otherwise, for a given recall (and assuming a fixed FPR), this will only be true at some specific value of prevalence—in particular, the value <img src="https://latex.codecogs.com/png.latex?%5Cmu%20'"> from before.</p>
<p>This also provides a way of thinking about how the quantifier bias relates to the precision-recall trade-off. For a high precision labeler, the ratio <img src="https://latex.codecogs.com/png.latex?%5Calpha/%5Cpi"> is small, and so the apparent prevalence will understate the true prevalence. It makes sense. A high precision classifier only produces apparent positives when it’s very sure, overlooking less certain cases, leading to an underestimation of the true prevalence. The opposite is true for a high recall labeler: <img src="https://latex.codecogs.com/png.latex?%5Calpha/%5Cpi"> is large, and so <img src="https://latex.codecogs.com/png.latex?%5Cell%20%3E%20%5Cmu">. With a high recall labeler we are casting a wide net, including many actual negatives in our estimation of prevalence.</p>
</section>
</section>
<section id="some-discussion-and-proposed-solutions-from-the-literature" class="level3">
<h3 class="anchored" data-anchor-id="some-discussion-and-proposed-solutions-from-the-literature">Some discussion and proposed solutions from the literature</h3>
<p>When the source of the imperfect labels is a machine learning model, the problem of estimating the true prevalence has sometimes been called <a href="https://en.wikipedia.org/wiki/Quantification_(machine_learning)">Quantification</a>—a secret third brother of the classical supervised tasks of Regression and Classification. In this context, the naive approach using <img src="https://latex.codecogs.com/png.latex?%5Cell"> as a proxy for the true prevalence has been called the “Classify And Count” method by Forman (2008), who proposes an alternative method which corrects the bias, appropriately called the “Adjusted Classify-And-Count” (ACC) method. Given <img src="https://latex.codecogs.com/png.latex?%5Calpha"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">, the ACC estimator is obtained by simply solving the equation <img src="https://latex.codecogs.com/png.latex?%5Cell%20=%20%5Cbeta%20+%20(%5Calpha%20-%20%5Cbeta)%20%5Cmu"> for <img src="https://latex.codecogs.com/png.latex?%5Cmu">, leading to:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%20%5Cmu_%7BACC%7D=%5Cfrac%7B%5Cell%20-%20%5Cbeta%7D%7B%5Calpha%20-%20%5Cbeta%7D"></p>
<p>This is not the first time this problem has been noticed. Rogan and Gladen (1978) discuss how the prevalence of diseases can be incorrectly estimated when we rely on imperfect tests, and propose the following correction, thereafter known as the Rogan-Gladen estimator.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%20%5Cmu%20_%7BRG%7D%20=%20%5Cfrac%7B%5Cell%20+%20%5Ctext%7BSpecificity%7D%20-%201%7D%7B%5Ctext%7BSensitivity%7D%20+%20%5Ctext%7BSpecificity%7D%20-%201%7D"></p>
<p>“Specificity” and “sensitivity” are more frequently used to describe medical tests than true and false positive rates, but it turns out this is actually exactly equivalent to the ACC correction. Sensitivity is just a synonym for recall, and the definition of specificity is <img src="https://latex.codecogs.com/png.latex?P(L=0%20%7C%20Y%20=%200)">, which happens to be equal to <img src="https://latex.codecogs.com/png.latex?1%20-%20%5Cbeta">. Substituting these in to <img src="https://latex.codecogs.com/png.latex?%5Chat%20%5Cmu_%7BRG%7D">, you get the exact formula for <img src="https://latex.codecogs.com/png.latex?%5Chat%20%5Cmu%20_%7BACC%7D">. Funny how things are discovered and rediscovered. I’m sure someone must have noticed this connection before, but I haven’t actually seen it written anywhere.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>There’s a lot more to talk about here. There are many other methods which are more sophisticated than ACC/Rogan-Gladen, and we haven’t even touched on the notion of a confidence interval here. And the solution above really does just kick the can down the road: it’s no easier in general to obtain reliable estimates of the TPR and FPR than it is to estimate prevalence directly. There’s a tricky resource optimization problem lurking here: is it cheaper to get high quality TPR and FPR estimates in order to build a Quantifier, or would you be better off just estimating prevalence directly by more traditional means? This is also not the only thing that makes it hard to estimate how many of what kind of posts there are. All kinds of other issues like severe class imbalance, non-response bias, and others all arise and compete and interact with each other to make it a very complicated task.</p>
<p>These are all perhaps topics for a future post, but the goal of this post has just been to raise awareness of this one tricky issue that I’ve rarely seen discussed, especially given its apparent ubiquity across different fields, and its annoyingness in my own day-to-day life.</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ul>
<li><p>Forman G. Quantifying counts and costs via classification. Data Mining and Knowledge Discovery. 2008 Oct;17:164-206.</p></li>
<li><p>Rogan WJ, Gladen B. Estimating prevalence from the results of a screening test. American journal of epidemiology. 1978 Jan 1;107(1):71-6.</p></li>
</ul>


</section>

 ]]></description>
  <guid>https://colin-fraser.net/posts/2024-11-12-estimating-how-many-there-are-of-something-using-a-classifier/</guid>
  <pubDate>Thu, 14 Nov 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Making Decisions With Classifiers</title>
  <dc:creator>Colin Fraser</dc:creator>
  <link>https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/</link>
  <description><![CDATA[ 




<section id="tldr" class="level1">
<h1>TL;DR</h1>
<ul>
<li>How to balance false positive rates and false negative rates is a problem that confronts anyone who does anything with machine learning, or even predicting more generally</li>
<li>In this post, I explore how to make an optimal choice of FPR and FNR given your relative valuation of a false positive and a false negative</li>
<li>I found it surprising that the optimal choice is determined <em>not only</em> by your preferences over false positives and false negatives, but also by the probability distribution of the positive class</li>
<li>Specifically, I show that if <img src="https://latex.codecogs.com/png.latex?%5Cpi"> is the prevalence of the positive class and <img src="https://latex.codecogs.com/png.latex?u_%7BTP%7D,%20u_%7BTN%7D,%20u_%7BFP%7D,%20u_%7BFN%7D"> are utilities associated with True Positives, True Negatives, False Positives, and False Negatives, then you should choose a point on the ROC curve which satisfies</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BROC%20Slope%7D%20=%20%5Cfrac%7B1-%5Cpi%7D%7B%5Cpi%7D%20%5Cfrac%7Bu_%7BTN%7D-u_%7BFP%7D%7D%7Bu_%7BTP%7D-u_%7BFN%7D%7D"></p>
<ul>
<li>I go through a worked example of how one might hypothetically go about applying this rule</li>
<li>I also show how you can use this rule to derive your implicit preferences if you know the ROC slope and prevalence of the positive class</li>
</ul>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In a recent Slack thread at my work, a question was posed about whether we should prefer precision to accuracy. A long discussion ensued which eventually shifted to discussing the tradeoff between types of errors in predictive models. It’s fairly well known that there is a trade-off between preventing false positives and allowing false negatives. Metaphorically, you can cast a very wide net, and you’ll catch a lot of fish, but also a lot of boots. Or you can fish with a spear, which will get you a lot less boots, but also a lot less fish. Casting a wide net gets you high recall, but the price you pay is a high false positive rate. Spearfishing gets you a much lower false positive rate, but lower recall, too.</p>
<p>This tradeoff is everywhere. In almost any situation where you are using some automated process to identify some class of thing, you’ll be faced with this tradeoff. This applies to systems that make guesses about whether your login attempt to a website is fraudulent, whether your image is contained in a photograph, whether your Facebook post is against Facebook’s community standards, whether you’re likely to renew your cell phone contract, and in all kinds of other ways that we’re not even aware of. In each of these cases, the engineers building the systems have to made a decision about how many false positives they want to permit.</p>
<p>It’s easy to see that this tradeoff always exists. You could stop all fraudulent logins to a website by preventing any user from ever signing on—this would get you 100% recall, but it would also get you a 100% false positive rate. You could achieve a 0% false positive rate by never flagging any signin attempt as fraudulent, then you’d have a 0% true positive rate. If you’re building the fraudulent signin detector, you have to pick somewhere in between.</p>
<p>It’s intuitive—and correct—that the best point for you will depend on your preferences over false positives and false negatives. If false positives are expensive, you should opt for a lower false positive rate; and vice versa. However, the conversation often just ends there. I’ve personally not seen (and I went looking) precise guidance on how to actually choose the best point along that continuum. This is what sent me on a quest to learn more about how to choose the best point along that frontier.</p>
</section>
<section id="definitions-and-notation" class="level1">
<h1>Definitions and notation</h1>
<p>Let <img src="https://latex.codecogs.com/png.latex?Y%5Cin%5C%7B0,1%5C%7D"> be the real world event about which we are making predictions, and <img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D%5Cin%5C%7B0,1%5C%7D"> the prediction made by our binary classifier. When a classifier predicts that a unit is positive, I’ll say that the unit has been <em>flagged</em>. When a unit is flagged, there are two possibilities: either the unit is actually a positive, in which case we have a <strong>true positive</strong>, or the unit is actually a negative, in which case we have a <strong>false positive</strong>. Similarly, when a unit is not flagged, we end up either with a <strong>true negative</strong> or a <strong>false negative</strong>. This is summarized by the <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>.</p>
<section id="prevalence" class="level3">
<h3 class="anchored" data-anchor-id="prevalence">Prevalence</h3>
<p>The prevalence of the event is the frequency with which it occues. Formally, the prevalence of <img src="https://latex.codecogs.com/png.latex?Y"> is equal to <img src="https://latex.codecogs.com/png.latex?P(Y=1)">. Since <img src="https://latex.codecogs.com/png.latex?Y"> is a binary random variable, this can also be expressed as <img src="https://latex.codecogs.com/png.latex?E%5BY%5D">. I will denote prevalence by <img src="https://latex.codecogs.com/png.latex?%5Cpi">.</p>
</section>
<section id="true-positive-rate-false-positive-rate-and-the-roc-curve" class="level3">
<h3 class="anchored" data-anchor-id="true-positive-rate-false-positive-rate-and-the-roc-curve">True Positive Rate, False Positive Rate, and the ROC Curve</h3>
<p>Every classifier, and indeed every machine learning model or predictive process, makes mistakes. When deciding whether to use a model, you need to balance the expected benefit of the correct predictions against the expected cost of the mistakes. One way to conceptualize these rates that I’ll use for the rest of this post is the True Positive Rate (TPR) and the False Positive Rate (FPR). TPR is the probability that the classifier correctly flags a randomly selected positive case, and FPR is the probability that the classifier <em>incorrectly</em> flags a randomly selected negative case.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Ctext%7BTPR%7D%20&amp;=%20P(%5Chat%7BY%7D=1%7CY=1)%20%5C%5C%0A%5Ctext%7BFPR%7D%20&amp;=%20P(%5Chat%7BY%7D=1%7CY=0)%20%5C%5C%0A%5Cend%7Balign%7D%0A"></p>
<p>FPR is often also referred to as the Type I error rate and denoted by the Greek letter <img src="https://latex.codecogs.com/png.latex?%5Calpha">. I’ll follow that convention here. TPR also has another name: recall. For that reason I will denote TPR by the letter <img src="https://latex.codecogs.com/png.latex?r">. (TPR is also sometimes referred to as the “sensitivity” of the classifier).</p>
</section>
<section id="the-roc-curve" class="level3">
<h3 class="anchored" data-anchor-id="the-roc-curve">The ROC Curve</h3>
<p>In practice, it is often possible to tune a model in order to obtain a desired FPR and TPR. A typical way that this can be done is when the model outputs a score. Choosing to predict positive when the score is above some threshold <img src="https://latex.codecogs.com/png.latex?t"> will lead to a certain pair of TPR and FPR. Varying <img src="https://latex.codecogs.com/png.latex?t"> will lead to different corresponding pairs of TPR and FPR. Plotting TPR as a function of FPR gives the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">Receiver Operating Characteristic (ROC) curve</a>, which is frequently used in the evaluation of machine learning models.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The figure above shows the ROC curve for two hypothetical models. It is common to plot the line <img src="https://latex.codecogs.com/png.latex?y=x"> on ROC plots—this represents the theoretical worst possible model, equivalent to simply flipping a coin. Models that outperform random guessing occupy the space above the 45-degree line. A theoretical perfect model would live at the point <img src="https://latex.codecogs.com/png.latex?(0,1)">: a model that flags every positive (TPR=1), while never incorrectly flagging a negative (FPR=0). In this case, the orange model is strictly better than the purple model, because at any given FPR, it can achieve a higher TPR. The closer the ROC curve is to the point (0,1), the better the model performs overall.</p>
</section>
<section id="utility-and-expected-utility" class="level2">
<h2 class="anchored" data-anchor-id="utility-and-expected-utility">Utility and Expected Utility</h2>
<section id="costs-and-benefits-of-predicting" class="level3">
<h3 class="anchored" data-anchor-id="costs-and-benefits-of-predicting">Costs and Benefits of Predicting</h3>
<p>As discussed above, when we use the classifier to make a prediction, there are four potential outcomes, which I’ll denote by <img src="https://latex.codecogs.com/png.latex?TP">, <img src="https://latex.codecogs.com/png.latex?TN">, <img src="https://latex.codecogs.com/png.latex?FP">, <img src="https://latex.codecogs.com/png.latex?FN">. Each of these outcomes has some cost or benefit associated with it, which I’ll describe as utilities, <img src="https://latex.codecogs.com/png.latex?u_%7BTP%7D">, <img src="https://latex.codecogs.com/png.latex?u_%7BTN%7D">, <img src="https://latex.codecogs.com/png.latex?u_%7BFP%7D">, <img src="https://latex.codecogs.com/png.latex?u_%7BFN%7D">. This utility function is reminiscent of a Payoff Matrix from elementary game theory.</p>
<table class="table-no-highlight">
<thead>
<tr>
<th colspan="4">
Classifier Payoff Matrix
</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="2" rowspan="2">
</td>
<td colspan="2">
<strong>Predicted</strong>
</td>
</tr>
<tr>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D=1">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D=0">
</td>
</tr>
<tr>
<td rowspan="2">
<strong>Actual</strong>
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?Y=1">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?u_%7BTP%7D">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?u_%7BFN%7D">
</td>
</tr>
<tr>
<td>
<img src="https://latex.codecogs.com/png.latex?Y=0">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?u_%7BFP%7D">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?u_%7BTN%7D">
</td>
</tr>
</tbody>
</table>
<p>The utilities associated with each outcome can vary significantly by domain. Fo instance, if the classifier is a COVID-19 test, then we might attach a higher cost to a false negative than to a false positive. While both are certainly costly, the damage done by a false positive is likely confined to the individual, who may be forced to unnecessarily quarantine—whereas, a false negative may lead the individual to infect others.</p>
<table class="table-no-highlight">
<thead>
<tr>
<th colspan="4">
Hypothetical COVID-19 Test Payoff Matrix
</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="2" rowspan="2">
</td>
<td colspan="2">
<strong>Predicted</strong>
</td>
</tr>
<tr>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D=1">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D=0">
</td>
</tr>
<tr>
<td rowspan="2">
<strong>Actual</strong>
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?Y=1">
</td>
<!-- TP -->
<td>
<img src="https://latex.codecogs.com/png.latex?1">
</td>
<!-- FN -->
<td>
<img src="https://latex.codecogs.com/png.latex?-10">
</td>
</tr>
<tr>
<td>
<img src="https://latex.codecogs.com/png.latex?Y=0">
</td>
<!-- FP -->
<td>
<img src="https://latex.codecogs.com/png.latex?-1">
</td>
<!-- TN  -->
<td>
<img src="https://latex.codecogs.com/png.latex?1">
</td>
</tr>
</tbody>
</table>
<p>On the other hand, many for many applications a false positive might be significantly more costly than a false negative. For example, imagine the model behind Face ID on an iPhone. A false negative means that the model incorrectly fails to identify your face as a match to what you’ve stored. This causes annoyance; you may have to scan again, or even unlock your phone by an alternate method. However, a false positive means that a face which is not yours was able to unlock the phone. This is presumably significantly worse.</p>
<table class="table-no-highlight">
<thead>
<tr>
<th colspan="4">
Face ID Payoff Matrix
</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="2" rowspan="2">
</td>
<td colspan="2">
<strong>Predicted</strong>
</td>
</tr>
<tr>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D=1">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D=0">
</td>
</tr>
<tr>
<td rowspan="2">
<strong>Actual</strong>
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?Y=1">
</td>
<!-- TP -->
<td>
<img src="https://latex.codecogs.com/png.latex?1">
</td>
<!-- FN -->
<td>
<img src="https://latex.codecogs.com/png.latex?-1">
</td>
</tr>
<tr>
<td>
<img src="https://latex.codecogs.com/png.latex?Y=0">
</td>
<!-- FP -->
<td>
<img src="https://latex.codecogs.com/png.latex?-10">
</td>
<!-- TN  -->
<td>
<img src="https://latex.codecogs.com/png.latex?1">
</td>
</tr>
</tbody>
</table>
</section>
<section id="expected-utility" class="level3">
<h3 class="anchored" data-anchor-id="expected-utility">Expected Utility</h3>
<p>We can combine the entries in the payoff matrix to obtain an <em>expected utility</em> <img src="https://latex.codecogs.com/png.latex?U"> of making a prediction. This represents the average overall benefit or cost that we expect to receive (or incur) by using the model for prediction.</p>
<p><img src="https://latex.codecogs.com/png.latex?U%20=%20P(TP)u_%7BTP%7D%20+%20P(TN)u_%7BTN%7D%20+%20P(FP)u_%7BFP%7D%20+%20P(FN)u_%7BFN%7D%20"></p>
<p>It is useful to rewrite this in terms of more commonly used classifier evaluation metrics. To do this, I’ll show how each probability in this term can be expressed as a combination of the prevalence <img src="https://latex.codecogs.com/png.latex?%5Cpi">, the TPR <img src="https://latex.codecogs.com/png.latex?r">, and the FPR <img src="https://latex.codecogs.com/png.latex?%5Calpha">.</p>
</section>
<section id="prediction-outcome-probability-identities" class="level3">
<h3 class="anchored" data-anchor-id="prediction-outcome-probability-identities">Prediction Outcome Probability Identities</h3>
<p><img src="https://latex.codecogs.com/png.latex?P(TP)%20=%20r%20%5Cpi">. This is easily derivable from the definition.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7DP(TP)%20&amp;=%20P(%5Chat%7BY%7D=1,Y=1)%20%5C%5C%0A&amp;=P(%5Chat%7BY%7D=1%7CY=1)P(Y=1)%20%5C%5C%0A&amp;=r%5Cpi%0A%5Cend%7Balign%7D%0A"> This makes some intuitive sense as well: the probability of a True Positive is equal to the rate at which the model correctly flags positives, multiplied by the prevalence of positives.</p>
<p><img src="https://latex.codecogs.com/png.latex?P(TN)%20=%20(1-%5Cpi)(1-%5Calpha)">. This can also be derived from the definitions.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7DP(TN)%20&amp;=%20P(%5Chat%7BY%7D=0,Y=0)%20%5C%5C%0A&amp;=P(%5Chat%7BY%7D=0%7CY=0)P(Y=0)%20%5C%5C%0A&amp;=%5Cleft(1-P(%5Chat%7BY%7D=1%7CY=0)%5Cright)(1-%5Cpi)%20%5C%5C%0A&amp;=(1-%5Calpha)(1-%5Cpi)%0A%5Cend%7Balign%7D%0A"></p>
<p>This is similar. It says that the probability of a True Negative is equal to the probability of <em>not</em> mis-flagging a negative, which is <img src="https://latex.codecogs.com/png.latex?1-%5Calpha">, multiplied by the probability of a negative, <img src="https://latex.codecogs.com/png.latex?1-%5Cpi">.</p>
<p><img src="https://latex.codecogs.com/png.latex?P(FP)%20=%20%5Calpha(1-%5Cpi)"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7DP(FP)%20&amp;=%20P(%5Chat%7BY%7D=1,Y=0)%20%5C%5C%0A&amp;=P(%5Chat%7BY%7D=1%7CY=0)P(Y=0)%20%5C%5C%0A&amp;=%5Calpha(1-%5Cpi)%0A%5Cend%7Balign%7D%0A"> This one is simple, it says that the probability of a false positive is the False Positive Rate multiplied by the prevalence of negatives.</p>
<p><img src="https://latex.codecogs.com/png.latex?P(FN)%20=%20(1-r)%5Cpi"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0AP(FN)%20&amp;=%20P(%5Chat%7BY%7D=0,Y=1)%20%5C%5C%0A&amp;=%20P(%5Chat%7BY%7D=0%7CY=1)P(Y=1)%20%5C%5C%0A&amp;=%20%5B1-P(%5Chat%7BY%7D=1%7CY=1)%5DP(Y=1)%20%5C%5C%0A&amp;=%20(1-r)%5Cpi%0A%5Cend%7Balign%7D%0A"></p>
<p>Similarly, this says that the probability of a False Negative is equal to the probability of failing to flag a positive, <img src="https://latex.codecogs.com/png.latex?1-r">, multiplied by the prevalence of positives.</p>
</section>
</section>
</section>
<section id="maximizing-expected-utility-along-the-roc-curve" class="level1">
<h1>Maximizing Expected Utility Along the ROC Curve</h1>
<p>Using the identities from the last section, we can rewrite the expected utility as follows.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AU=%5Cpi%20u_%7BTP%7Dr(%5Calpha)%20+%20%5Cpi%20u_%7BFN%7D(1-r(%5Calpha))%20+%20(1-%5Cpi)u_%7BFP%7D%5Calpha%20%20+%20(1-%5Cpi)u_%7BTN%7D(1-%5Calpha)%0A"></p>
<p>Given an ROC curve, a payoff matrix, and a prevalence, this allows us to plot the expected utility of a prediction alongside the ROC curve. The following plot shows a hypothetical ROC curve (parameterized as <img src="https://latex.codecogs.com/png.latex?r=%5Csqrt%7B2%5Calpha-%5Calpha%5E2%7D"> for simplicity) along with the expected utility at each point when prevalence is 0.5.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>To find the point along the ROC curve where expected utility is maximized, we can take the derivative of expected utility with respect to <img src="https://latex.codecogs.com/png.latex?%5Calpha">, and find the point where it’s equal to 0 (assuming that the ROC curve is well-behaved, i.e.&nbsp;looks something like the characteristic bow-shape, is monotonic, etc.).</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BdU%7D%7Bd%5Calpha%7D%20=%20%5Cpi%20(u_%7BTP%7D%20-%20u_%7BFN%7D)r'(%5Calpha)%20+%20(1-%5Cpi)(u_%7BTN%7D-u_%7BFP%7D)%20"></p>
<p>Setting this expression equal to <img src="https://latex.codecogs.com/png.latex?0"> and solving for <img src="https://latex.codecogs.com/png.latex?r'(%5Calpha)"> gives an interesting condition for the utility-maximizing point.</p>
<p><img src="https://latex.codecogs.com/png.latex?r'(%5Calpha)%20=%20%5Cfrac%7B1-%5Cpi%7D%7B%5Cpi%7D%5Cfrac%7Bu_%7BTN%7D-u_%7BFP%7D%7D%7Bu_%7BTP%7D-u_%7BFN%7D%7D"></p>
<p>This equation tells us what must be true about the slope of the ROC curve at the point which maximizes the expected utility. Since well-behaved ROC curves look similar to each other, this gives us some heuristics that will be true for most models.<img src="https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/images/ROC1.png" class="img-fluid"></p>
<p>The ROC curve starts steep and becomes flat. Thus, when the right hand side of this equation is large, then the optimal point is somewhere on the left side of the ROC curve, where the curve is steep and the false positive rate is low. Conversely, when the right hand side is small, then the optimal point is somewhere on the right side of the ROC curve, where the FPR is high and the curve is flatter.</p>
<p>The utility term in this expression can be simplified further by reframing the utility function. The numerator <img src="https://latex.codecogs.com/png.latex?u_%7BTN%7D-u_%7BFP%7D"> can be understood as the cost incurred by switching from a TN to a FP. In other words, it’s the marginal cost of incorrectly flagging a negative. I’ll call this the Marginal Cost of a False Positive (MCFP). Similarly, the denominator <img src="https://latex.codecogs.com/png.latex?u_%7BTP%7D-u_%7BFN%7D"> can be understood as the marginal cost of incorrectly failing to flag a positive, or the Marginal Cost of a False Negative (MCFN). To further simplify the expression, I’ll give this ratio MCFP/MCFN a name: the <em>Error Exchange Rate</em> (EER). This ratio represents the number of false positives that you would be willing to give up in order to prevent a false negative. In these terms, the optimality condition becomes</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7Dr'(%5Calpha)%20&amp;=%5Ctext%7BROC%20slope%7D%20%5C%5C%20&amp;=%0A%5Cfrac%7B1-%5Ctext%7Bprevalence%7D%7D%7B%5Ctext%7Bprevalence%7D%7D%5Cfrac%7B%5Ctext%7BMarginal%20Cost%20of%20a%20False%20Positive%7D%7D%7B%5Ctext%7BMarginal%20Cost%20of%20a%20False%20Negative%7D%7D%20%5C%5C%20&amp;=%5Cfrac%7B1-%5Cpi%7D%7B%5Cpi%7D%5Cfrac%7BMCFP%7D%7BMCFN%7D%20%5C%5C%20&amp;=%5Ctext%7Bodds%7D(Y=0)%20%5Ctimes%20EER%0A%5Cend%7Balign%7D"></p>
<section id="key-point-the-optimal-point-on-the-roc-curve-is-partly-determined-by-prevalence" class="level2">
<h2 class="anchored" data-anchor-id="key-point-the-optimal-point-on-the-roc-curve-is-partly-determined-by-prevalence">Key Point: The optimal point on the ROC curve is partly determined by prevalence</h2>
<p>One of the primary determinants of the optimal point along the ROC curve is the prevalence of the positive class. I always thought that the optimal choice of classifier would be determined solely by preferences over false positives and false negatives, but on reflection it makes sense: you should optimize with respect to your relative valuation of errors <em>and</em> with respect to the expected volume of errors that will be produced by your choice. This latter aspect is not determined by your valuation over errors, but rather by how common negatives are in the data.</p>
<p>Since the optimality condition sets the slope of the ROC curve <em>proportional</em> to the odds of a negative, the optimal slope as a function of prevalence always has the same shape. Differences in the valuation of errors simply scale the function vertically.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>To help understand how this manifests, the following plot shows a hypothetical ROC curve (defined as <img src="https://latex.codecogs.com/png.latex?r(%5Calpha)=%5Csqrt%7B2%5Calpha%20-%20%5Calpha%5E2%7D">, which is just a quarter-circle with radius 1 centered at (1,0)) along with the expected utilities and optimal points assuming the same utilities, but different prevalences. For simplicity I assume that EER=1, but alternative EERs would produce similar qualitative results.</p>
<p>(Fun calculus exercise: show that these assumptions imply that the optimal recall as a function of prevalence is equal to <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E*%20=%20%5Cfrac%7B2%5Cpi%5E2%20-%20%5Csqrt%7B2%5Cpi%20%5E4%20-%206%5Cpi%20%5E3%20+%207%5Cpi%5E2%20-%204%5Cpi%20+%201%7D%7D%7B2%5Cpi%5E2%20-%202%5Cpi%20+%201%7D">)</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/index_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Although this exercise is contrived (by me), I still found the results a little surprising when I first went through this. Since it’s well known that ROC curves are “better” the closer they get to the point (1,0), I would have expected that when we are indifferent between a false positive and a false negative, the optimal point should be at the point along the ROC curve that is closest to the point (1,0). But this is only true in the specific case where prevalence is 0.5. In general, the optimal point can be anywhere along the ROC curve—further to the left when prevalence is low, and further to the right when prevalence is high.</p>
<section id="the-optimal-point-on-the-roc-curve-is-determined-by-your-preferences" class="level3">
<h3 class="anchored" data-anchor-id="the-optimal-point-on-the-roc-curve-is-determined-by-your-preferences">The optimal point on the ROC curve is determined by your preferences</h3>
<p>The preferences, of course, also determine the optimal point along the ROC curve. The optimal point is fully determined by the prevalence and your EER, so you don’t actually even need to fully specify all four utility values—you just need to know the EER ratio.</p>
<p>The EER is the number of false positives you would need to prevent in order to allow a single false negative. If this is greater than 1, it’s because a false positive is costlier to you than a false negative, and vice versa. It can be challenging to come up with an exact value for this, but it can be instructive to try out some values, plug them in, and see what it does to the optimal ROC point.</p>
</section>
<section id="putting-it-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-together">Putting it together</h3>
<p>To choose the optimal point on the ROC curve, you need a prevalence and an EER. Since the function for deriving the optimal ROC slope from the prevalence and EER is pretty simple, it’s possible to derive some simple heuristics for developing intuition about optimal classifiers. When prevalence is low, <img src="https://latex.codecogs.com/png.latex?(1-%5Cpi)/%5Cpi"> is high. If EER is also high, then the optimal slope is given by a big number times a big number, which is a big number. This implies that for low prevalence and high EER, the optimal point will be somewhere towards the left side of the ROC curve, where it’s steep. A similar argument can be made when prevalence is high and EER is low.</p>
<table style="all: revert;">
<tbody>
<tr>
<td colspan="2" rowspan="2">
</td>
<td colspan="2">
EER
</td>
</tr>
<tr>
<td>
High
</td>
<td>
Low
</td>
</tr>
<tr>
<td rowspan="2">
Prevalence
</td>
<td>
High
</td>
<td>
Depends
</td>
<td>
High Recall, Low Precision
</td>
</tr>
<tr>
<td>
Low
</td>
<td>
Low Recall, High Precision
</td>
<td>
Depends
</td>
</tr>
</tbody>
</table>
<p>These rules of thumb do make some intuitive sense. When EER is high, it means that a False Positive is costlier than a False Negative. When prevalence is low, it means that there are lots of opportunities for false positives. So when both of those things are true, we should definely choose a classifier that produces a minimal volume of false positives—which would be one with high precision and low recall.</p>
<p>On the other hand, suppose we have high EER but also high prevalence, so we are in the top left quadrant. In this case, a false positive is costlier than a false negative, but there aren’t as many opportunities for false positives. There’s not enough information to determine whether we should choose higher precision or higher recall here; we’ll need to see the specific value.</p>
</section>
</section>
</section>
<section id="worked-example" class="level1">
<h1>Worked Example</h1>
<section id="deriving-implied-preferences" class="level2">
<h2 class="anchored" data-anchor-id="deriving-implied-preferences">Deriving implied preferences</h2>
<p>Throughout this piece, I’ve assumed that we have readily available values for the costs and benefits of hits and errors. In practice, this is rare. In most of my experience with machine learning applications, classifier thresholds have been chosen on the basis of an intuitive sense about the relative costs of errors, but without actually explicitly defining those costs and performing these assessments.</p>
<p>In these cases, it may be a useful exercise to apply these rules in the other direction: rather than taking the utility function and prevalence to select the optimal point along the ROC curve, we might assume that we’ve chosen the optimal point already, and use that point and the prevalence to derive the implied EER.</p>
<p>As an example, suppose you’ve got a model for predicting customer churn. Based on gut feeling, you’ve calibrated the model to have a 10% false positive rate, which gets 44% recall. Suppose further that the population customer churn rate is 5%.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We can use this information to calculate the EER that’s implied by these numbers. The trickiest part here is determining the slope of the ROC curve at the point we’ve selected. For this example I cheated a little bit by specifically contriving a nice ROC curve <img src="https://latex.codecogs.com/png.latex?r%20=%20%5Csqrt%7B2%5Calpha%20-%20%5Calpha%5E2%7D">. It’s (relatively) easy to apply some calculus 101 and determine that the slope of that curve at <img src="https://latex.codecogs.com/png.latex?%5Calpha=0.1"> is about 2.06. In real life, it will be a little bit harder to find this slope, and the best approach for doing this might be the subject of a future post. But anyway, if we have the slope, then we just plug the numbers into the optimality condition to derive our implied relative valuation of False Positives and False Negatives.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0Ar'(%5Calpha)%20&amp;=%20%5Ctext%7Bodds%7D(Y=0)%20%5Ctimes%20EER%20%5C%5C%0A2.06%20&amp;=%20%5Cfrac%7B1-5%5C%25%7D%7B5%5C%25%7D%20%5Ctimes%20EER%20%5C%5C%0AEER%20&amp;=%20%5Cfrac%7B2.06%7D%7B19%7D%20=%200.108%0A%5Cend%7Balign%7D%0A"></p>
<p>An EER of less than 1 implies that we value false negatives more highly than false positives; in particular, it means that a false negative is worth about <img src="https://latex.codecogs.com/png.latex?1/1.08%5Capprox%209"> false negatives. Now we might take a step back and think about whether this makes sense for our particular application. Does this valuation make sense?</p>
</section>
<section id="finding-an-optimal-point-given-explicit-preferences" class="level2">
<h2 class="anchored" data-anchor-id="finding-an-optimal-point-given-explicit-preferences">Finding an optimal point given explicit preferences</h2>
<p>Suppose we use these predictions to send out a promotional offer to customers who we think are likely to churn. In this case, it might be feasible to explicitly price false positives and false negatives (this is, of course, rare!).</p>
<p>Say that the regular price of our service is \$1,000, and the promotional offer gives the customer \$100 off. Let’s also suppose that a customer who wasn’t going to cancel anyway would always take the offer if we give it to them, but a customer who would otherwise leave takes the offer half the time This is enough information to price the outcomes. A true positive means we send the offer to a customer who would have canceled. Half the time they take it, and the other half the cancel, so we get <img src="https://latex.codecogs.com/png.latex?U_%7BTP%7D=.5%5Ctimes%20900=450">. A true negative means we correctly withhold the offer from a customer who pays full price, so <img src="https://latex.codecogs.com/png.latex?U_%7BTN%7D=1000">. A false positive means we send the offer to a customer who was going to stay anyway, so <img src="https://latex.codecogs.com/png.latex?U_%7BFP%7D=900">, and finally a false negative means we miss sending the offer to a customer who leaves which gives us <img src="https://latex.codecogs.com/png.latex?U_%7BFN%7D=0">.</p>
<table class="table-no-highlight">
<thead>
<tr>
<th colspan="4">
Churn Model Payoff Matrix
</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="2" rowspan="2">
</td>
<td colspan="2">
<strong>Predicted</strong>
</td>
</tr>
<tr>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D=1">
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7BY%7D=0">
</td>
</tr>
<tr>
<td rowspan="2">
<strong>Actual</strong>
</td>
<td>
<img src="https://latex.codecogs.com/png.latex?Y=1">
</td>
<!-- TP -->
<td>
<img src="https://latex.codecogs.com/png.latex?450">
</td>
<!-- FN -->
<td>
<img src="https://latex.codecogs.com/png.latex?0">
</td>
</tr>
<tr>
<td>
<img src="https://latex.codecogs.com/png.latex?Y=0">
</td>
<!-- FP -->
<td>
<img src="https://latex.codecogs.com/png.latex?900">
</td>
<!-- TN  -->
<td>
<img src="https://latex.codecogs.com/png.latex?1000">
</td>
</tr>
</tbody>
</table>
<p>From here we can determine our actual EER:</p>
<p><img src="https://latex.codecogs.com/png.latex?EER%20=%20%5Cfrac%7BMCFP%7D%7BMCFN%7D%20=%20%5Cfrac%7B1000-900%7D%7B450-0%7D=0.222"> Since this differs from our implied EER, we’ve chosen a suboptimal point along the ROC curve. We should choose a point according to the optimality condition <img src="https://latex.codecogs.com/png.latex?r'(%5Calpha)%20=%20%5Cfrac%7B1-%5Cpi%7D%7B%5Cpi%7D%20%5Ctimes%20EER%20=%2019%20%5Ctimes%200.222%20=%204.22">. Since we are currently at a point where the slope is 2.06, we need to move along the curve towards a steeper region—which, given the shape of the curve, will be further to the left, where the false positive rate is lower.</p>
<p>The way to interpret this is that we are currently permitting too many false positives. Even though a single false positive is <em>less</em> costly (\$100) than a single false negative (\$450), since the prevalence is quite low, there are way more opportunities for false positives than false negatives. To account for this, we have to limit the false positive rate at the expense of recall.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/index_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>This type of result was a little bit surprising and counterintuitive to me when I first came across it. Intuitively I feel like if a false negative is more costly than a false positive, then I should be willing to give up some false positives in order in order to get a high recall. But in this example, we have a case where false negatives are costlier than false positives, and yet the optimal recall is only 0.03. The reason, again, is that it is not simply the relative valuation of false positives and false negatives that determines the total expected utility, <em>but also their relative frequencies</em>—which is determined by prevalence. Even if a single false positive doesn’t cost very much, a flood of false positives can add up. If my classifier is permitting a flood of false positives, I might be able to do better by choosing a lower false positive rate at the expense of permitting some false negatives.</p>
</section>
</section>
<section id="the-complicated-real-world" class="level1">
<h1>The Complicated Real World</h1>
<p>My purpose in going through all of this was to build my intuition and derive some heuristics for thinking about the TPR/FPR tradeoffs in typical cases. However, there is a lot left out. I won’t address these points here, but I want to at least acknowledge them.</p>
<section id="uncertainty-and-randomness" class="level2">
<h2 class="anchored" data-anchor-id="uncertainty-and-randomness">Uncertainty and Randomness</h2>
<p>In real life, you don’t actually know the <em>real</em> ROC curve, if such a thing is even well-defined. Models are evaluated on some test dataset, and the ROC curve displayes statistical estimates of the FPR and TPR with respect to the population from which the validation set is sampled. If you get a new test set, you’ll get a new ROC curve. If the test set is sampled from the population on which the model will be running, then the resulting ROC curve and other model diagnostic statistics should be representative of what will be observed online. However, this often isn’t the case: it’s often desirable to sample cases which is are representative of the population, but which exemplify certain edge cases or ambiguous cases that you want the model to learn. In this case, the ROC and prevalence measured in the test set may not give a good representation of the actual tradeoffs that you face when selecting a point along the ROC curve.</p>
<p>But even if the test set is representative of the population, we are still subject to uncertainty: it’s impossible to know the <em>true</em> TPR and FPR of your model. You can only know the performance on a sample of data, and you have to hope that this is a good estimate of its performance on the full population. Of course, there are ways to quantify your uncertainty and account for this—that’s basically the fundamental probablem of statistics—but it’s not at all <em>obvious</em> how. I don’t address this here, but it’ll probably make a good topic for some future hypothetical post.</p>
</section>
<section id="roc-curves-arent-that-nice" class="level2">
<h2 class="anchored" data-anchor-id="roc-curves-arent-that-nice">ROC curves aren’t that nice</h2>
<p>To make things straightforward (and partly out of some nostalgia for Calculus 101), I’ve worked with nice smooth ROC curves somewhat idealized for doing calculus with. In real life, ROC curves are derived from empirical data, and are drawn with step functions, meaning all the calculus stuff I’ve been doing here goes out the window.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/Roccurves.png" class="img-fluid figure-img"></p>
<figcaption>By <a href="https://en.wikipedia.org/wiki/User:BOR" class="extiw" title="en:User:BOR">BOR</a> at the <a href="https://en.wikipedia.org/wiki/" class="extiw" title="w:">English-language Wikipedia</a>, <a href="http://creativecommons.org/licenses/by-sa/3.0/" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=10714489">Link</a></figcaption>
</figure>
</div>
<p>It will be less straightforward to determine the optimal point along one of these real-world empirical ROC curves than what I’ve shown here, and how exactly to do it could be the subject of some future post. Nonetheless, the general intuition and principles should apply. When False Positives are expensive and prevalence is low, we should choose a point towards the left; when they are cheap and prevalence is high, we should choose a point towards the right. Holding everything else equal, when prevalence goes up, the optimal recall also goes up, and vice versa. But that brings me to the next complication.</p>
</section>
<section id="everything-else-isnt-equal" class="level2">
<h2 class="anchored" data-anchor-id="everything-else-isnt-equal">Everything Else Isn’t Equal</h2>
<p>I’ve mentioned a few times that holding everything else equal, when prevalence goes down then we should choose a point further to the left on the ROC curve. However, in a certain sense, this statement doesn’t actually make any sense. When prevalence changes, the TPR and FPR will also change—meaning, the ROC itself will change. It would actually be very unusual to see prevalence shift but keep the ROC entirely unchanged.</p>
<p>The point here isn’t to develop a general theory about the relationship between ROC’s, prevalence, and utility (maybe one day but not today), but simply to figure out how to think about a specific but very common case: you have a ROC curve, and you have (implicit or explicit) preferences over false positives and false negatives, and you need to choose a point along the ROC curve. This piece provides a way to conceptualize this problem, but doesn’t really explore what happens if the prevalence changes.</p>
</section>
<section id="constant-marginal-utility-is-probably-not-realistic-at-scale" class="level2">
<h2 class="anchored" data-anchor-id="constant-marginal-utility-is-probably-not-realistic-at-scale">Constant marginal utility is probably not realistic at scale</h2>
<p>One of the implicit assumptions I use throughout this post is that marginal utility is constant with respect to the number of predictions made. For instance, in the worked example I assume that every false positive has a net cost of \$100, so 1 false positive costs \$100, 100 false positives costs \$10,000, and a million false positives costs a hundred million dollars.</p>
<p>This is probably not realistic, and depending on the application, it probably becomes less and less realistic as you scale up. Suppose we have a model that prevents a user from signing in if it predicts that the sign-in attempt is fraudulent. A single false positive probably creates some annoyance for the user, and might cost us in terms of however much it costs to provide support to unlock the user’s account. But after three or four false positives, the user’s annoyance might boil over into a decision to stop trying to sign in at all, costing us an active user in the long term. In this case, the cost of a first false positive is somehow less than the cost of the second or third false positives. This type of cost function is not represented at all by this model, and could also be a topic for some future post.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Going through this exercise was extremely helpful for me. I don’t think I really had a strong intuition about this particular problem until I took the time to do this. It makes a lot of sense that we should choose a high precision classifier when false positives are expensive, and high recall when they are cheap, but the degree to which this is mediated by the underlying distribution of the positive class was not obvious to me at all prior to this adventure. I hope some other people out there feel similarly and that this can be a useful starting point for figuring out how to deal with classifiers in production when there are real costs to errors.</p>


</section>

 ]]></description>
  <guid>https://colin-fraser.net/posts/2021-10-18-making-decisions-with-classifiers/</guid>
  <pubDate>Mon, 18 Oct 2021 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
